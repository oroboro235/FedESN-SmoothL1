# this is for implementing the method of solving different problems

from torch.optim import Adam, SGD, Adagrad, Adadelta, RMSprop

# optimizers

# Newton method + backtracking line search



# Accelerated gradient descent(AGD) + SmoothL1

